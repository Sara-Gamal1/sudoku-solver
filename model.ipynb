{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "import numpy as np\n",
    "import pickle \n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn import cluster\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "sift = cv2.SIFT_create(nfeatures=100,contrastThreshold=0.01,nOctaveLayers=5)\n",
    "\n",
    " \n",
    "# Depending on library versions on your system, one of the following imports \n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42  \n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images,titles=None):\n",
    "    n_ims = len(images)\n",
    "    if titles is None: titles = ['(%d)' % i for i in range(1,n_ims + 1)]\n",
    "    fig = plt.figure()\n",
    "    n = 1\n",
    "    for image,title in zip(images,titles):\n",
    "        a = fig.add_subplot(1,n_ims,n)\n",
    "        if image.ndim == 2: \n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "        n += 1\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_ims)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(img):\n",
    "    \n",
    "    resized_img_1 = cv2.resize(img, (128*4, 64*4))\n",
    "\n",
    "    # creating hog features\n",
    "    ft = hog(resized_img_1, orientations=18, pixels_per_cell=(\n",
    "        8, 8), cells_per_block=(2, 2), visualize=False)\n",
    "    # console.log(ft)\n",
    "    ft = ft.flatten()\n",
    "    return ft.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = []\n",
    "def load_dataset():\n",
    "    path_to_dataset = r'binary'\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'jpg':\n",
    "            continue\n",
    "\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        if(des is not None):\n",
    "            for d in des:\n",
    "                dico.append(d)\n",
    "            \n",
    "        # show an update every 1,000 images\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "        # features.append(HOG(gray_img))\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 1000/7509\n",
      "[INFO] processed 2000/7509\n",
      "[INFO] processed 3000/7509\n",
      "[INFO] processed 4000/7509\n",
      "[INFO] processed 5000/7509\n",
      "[INFO] processed 6000/7509\n",
      "[INFO] processed 7000/7509\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    " # Load dataset with extracted features\n",
    "print('Loading dataset. This will take time ...')\n",
    "hog_features, labels = load_dataset()\n",
    "print('Finished loading dataset.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 2562936310.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 2577697908.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 2577468228.0\n",
      "[MiniBatchKMeans] Reassigning 7 cluster centers.\n",
      "Minibatch step 1/275: mean batch inertia: 41392.276823367516\n",
      "[MiniBatchKMeans] Reassigning 3 cluster centers.\n",
      "Minibatch step 2/275: mean batch inertia: 33002.573100598165, ewa inertia: 33002.573100598165\n",
      "Minibatch step 3/275: mean batch inertia: 31845.782621337163, ewa inertia: 32162.040922218246\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "Minibatch step 4/275: mean batch inertia: 31245.582950026735, ewa inertia: 31496.136050445893\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "Minibatch step 5/275: mean batch inertia: 31120.028843968576, ewa inertia: 31222.85388290357\n",
      "Minibatch step 6/275: mean batch inertia: 31132.476376653394, ewa inertia: 31157.184944952478\n",
      "Minibatch step 7/275: mean batch inertia: 30838.961746520366, ewa inertia: 30925.9617087792\n",
      "Minibatch step 8/275: mean batch inertia: 30872.266237151136, ewa inertia: 30886.9461977201\n",
      "Minibatch step 9/275: mean batch inertia: 30651.484778914808, ewa inertia: 30715.858258442393\n",
      "Minibatch step 10/275: mean batch inertia: 30632.153329245954, ewa inertia: 30655.037662370778\n",
      "Minibatch step 11/275: mean batch inertia: 30399.072390750156, ewa inertia: 30469.05147800805\n",
      "Minibatch step 12/275: mean batch inertia: 30571.4448387426, ewa inertia: 30543.45122117056\n",
      "Minibatch step 13/275: mean batch inertia: 30594.508904379018, ewa inertia: 30580.550096428982\n",
      "Minibatch step 14/275: mean batch inertia: 30403.853428727314, ewa inertia: 30452.16104268207\n",
      "Minibatch step 15/275: mean batch inertia: 30376.811434838582, ewa inertia: 30397.411482449505\n",
      "Minibatch step 16/275: mean batch inertia: 30361.139891848263, ewa inertia: 30371.056287098276\n",
      "Minibatch step 17/275: mean batch inertia: 30456.318712513763, ewa inertia: 30433.008570984828\n",
      "Minibatch step 18/275: mean batch inertia: 30112.64368642262, ewa inertia: 30200.229170458013\n",
      "Minibatch step 19/275: mean batch inertia: 30380.36008258134, ewa inertia: 30331.113570593556\n",
      "Minibatch step 20/275: mean batch inertia: 30447.818700037762, ewa inertia: 30415.912346550387\n",
      "Minibatch step 21/275: mean batch inertia: 30252.00379362065, ewa inertia: 30296.815227824838\n",
      "Minibatch step 22/275: mean batch inertia: 30180.502904020068, ewa inertia: 30212.30186723554\n",
      "Minibatch step 23/275: mean batch inertia: 30253.308523309646, ewa inertia: 30242.09759479034\n",
      "Minibatch step 24/275: mean batch inertia: 30074.158879645413, ewa inertia: 30120.072131574852\n",
      "Minibatch step 25/275: mean batch inertia: 30298.34340524614, ewa inertia: 30249.60530520299\n",
      "Minibatch step 26/275: mean batch inertia: 30140.17036706461, ewa inertia: 30170.089101885784\n",
      "Minibatch step 27/275: mean batch inertia: 30199.758597606204, ewa inertia: 30191.647168204974\n",
      "Minibatch step 28/275: mean batch inertia: 30137.64236265275, ewa inertia: 30152.40689291952\n",
      "Minibatch step 29/275: mean batch inertia: 30053.74240267933, ewa inertia: 30080.71657354266\n",
      "Minibatch step 30/275: mean batch inertia: 30036.440139631264, ewa inertia: 30048.545002187566\n",
      "Minibatch step 31/275: mean batch inertia: 30048.528109625222, ewa inertia: 30048.532727931786\n",
      "Minibatch step 32/275: mean batch inertia: 30154.738698866393, ewa inertia: 30125.702740744862\n",
      "Minibatch step 33/275: mean batch inertia: 30165.038181144166, ewa inertia: 30154.284151120475\n",
      "Minibatch step 34/275: mean batch inertia: 30278.457572069277, ewa inertia: 30244.509440728372\n",
      "Minibatch step 35/275: mean batch inertia: 30224.36753796473, ewa inertia: 30229.87419112167\n",
      "Minibatch step 36/275: mean batch inertia: 29982.65928361309, ewa inertia: 30050.24608347255\n",
      "Minibatch step 37/275: mean batch inertia: 30332.981374422772, ewa inertia: 30255.68355401526\n",
      "Minibatch step 38/275: mean batch inertia: 30183.320255970888, ewa inertia: 30203.103867693113\n",
      "Minibatch step 39/275: mean batch inertia: 30133.767680044566, ewa inertia: 30152.72370128474\n",
      "Minibatch step 40/275: mean batch inertia: 30052.230382294812, ewa inertia: 30079.704541948995\n",
      "Minibatch step 41/275: mean batch inertia: 30083.241107617603, ewa inertia: 30082.2742356824\n",
      "Converged (lack of improvement in inertia) at step 41/275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_dataset = r'binary'\n",
    "\n",
    "img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "k = 1600\n",
    "\n",
    "batch_size = np.size(os.listdir(path_to_dataset)) * 3\n",
    "kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, verbose=1).fit(dico)\n",
    "with open( 'kmeans.pkl', 'wb') as file:\n",
    "          pickle.dump(kmeans, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "histo_list = []\n",
    "kmeans.verbose = False\n",
    "\n",
    "path_to_dataset = r'binary'\n",
    "labels = []\n",
    "img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'jpg':\n",
    "            continue\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "        histo = np.zeros(k)\n",
    "\n",
    "        nkp = np.size(kp)\n",
    "        if(des is not None):\n",
    "            for d in des:\n",
    "                idx = kmeans.predict([d])\n",
    "                histo[idx] += 1/nkp # Because we need normalized histograms, I prefere to add 1/nkp directly\n",
    "\n",
    "        histo_list.append(histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sift_classifier = svm.SVC(C=1.0, gamma='scale')\n",
    "sift_classifier.fit(histo_list, labels)\n",
    "with open( 'sift_classifier.pkl', 'wb') as file:\n",
    "          pickle.dump(sift_classifier, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN= KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(histo_list, labels)\n",
    "with open( 'KNN.pkl', 'wb') as file:\n",
    "          pickle.dump(KNN, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will test all our classifiers on a specific feature set\n",
    "# hog_classifier = svm.SVC(C=1.0, gamma='scale')\n",
    "\n",
    "# hog_classifier.fit(hog_features, labels)\n",
    "# with open( 'hog_classifier.pkl', 'wb') as file:\n",
    "#           pickle.dump(hog_classifier, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
